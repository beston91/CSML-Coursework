{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c0475a",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76654b2a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56b639",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff01e0c5f082de3fbc99eabd80a886f7",
     "grade": false,
     "grade_id": "cell-b05b8a76a1dcf57b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf80f5",
   "metadata": {},
   "source": [
    "# Fitting a variational auto-encoder\n",
    "\n",
    "In part one of this coursework, you will fit a variational autoencoder to the MNIST dataset.\n",
    "\n",
    "We'll download the data and plot a few images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a255ae8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0a394cbb4e520fb642179c72edec0d3",
     "grade": false,
     "grade_id": "cell-599f19b69ac4861f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset and apply necessary transformations\n",
    "\n",
    "mnist_transforms = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.round())])\n",
    "mnist_dataset = MNIST(root='data/', download=True, train=True, transform=mnist_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3346c7",
   "metadata": {},
   "source": [
    "## Data format\n",
    "\n",
    "Each MNIST image $x_i$ is a **binary image** with shape `(1, 28, 28)`, i.e.\n",
    "\n",
    "$$x_i \\in \\{0, 1\\}^{1\\times28\\times28}$$\n",
    "\n",
    "The first dimension is the number of \"channels\". It is `1` because these are black-and-white images. (For color images, there are three channels.)\n",
    "\n",
    "The convention in pytorch is for images to be `[channels][rows][columns]`. This is different than numpy, which you might be familiar with, which instead has `[rows][columns][channels]`. We will have to call `torch.permute` to re-arrange the axes, sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ff093",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "X_random_batch, y_random_batch = next(iter(train_loader))\n",
    "plt.imshow(make_grid(X_random_batch, nrow=20).permute(1,2,0));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06aa290",
   "metadata": {},
   "source": [
    "# TASK 1 (6 points + 2 bonus): Define encoder and decoder networks\n",
    "\n",
    "We have data $x_i$ and will learn a latent representation $z_i \\in \\mathbb{R}^{D}$.\n",
    "\n",
    "To do this we define two different networks, each as an `nn.Module` instance.\n",
    "\n",
    "\n",
    "* The **Encoder** defines an inference network. It takes a batch of data instances as inputs, and returns a probability distribution $q(z_i | x_i)$ for each $x_i$ in the batch.\n",
    "* The **Decoder** defines the likelihood in a generative model. It takes a latent value as input, and returns a probability distribution $p(x_i | z_i)$.\n",
    "\n",
    "## You can use any network architecture you think is appropriate.\n",
    "\n",
    "Part of your job here is to decide.\n",
    "\n",
    "* You can get up to 6 points (3 points each) for having an implementation of any non-linear model that runs properly and can be used to fit some sort of VAE. If it doesn't quite work (or if you just implement a linear model, or a poor choice of architecture which leads to bad results) then you will get partial credit.\n",
    "\n",
    "* You can get 2 extra credit points if you do something \"interesting\": this would in particular mean architectures which perform better than a feed-forward network, while having fewer parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f98a0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b213a3f766e9acd6cb49891f5f9d5285",
     "grade": false,
     "grade_id": "vae-enc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, D_latent):\n",
    "        \"\"\" \n",
    "        Initialize the Encoder `nn.Module`.\n",
    "        \n",
    "        This will operate on inputs of shape (batch_size, 1, 28, 28).\n",
    "        \n",
    "        INPUTS:\n",
    "        D_latent: size of latent space (integer)\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.D_latent = D_latent\n",
    "        \n",
    "        # Define any networks (which extend 'nn.Module') here:\n",
    "        # Anything that has parameters should be defined here!\n",
    "        \n",
    "        # e.g., a linear layer could be defined as \n",
    "        # self.fc = nn.Linear(784, D_latent)\n",
    "        #\n",
    "        # Later on in the `.forward` method, you then can call this.\n",
    "        #\n",
    "        # Some ``useful'' layers include:\n",
    "        #\n",
    "        # nn.Linear, nn.ReLU, nn.Conv2d, nn.Sequential, nn.Softplus, ...\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" Return a distribution q(z | x). \n",
    "        \n",
    "        Remember you can access and variables or `nn.Module` instances defined in the constructor.\n",
    "        \n",
    "        INPUT:\n",
    "        X    : torch.FloatTensor containing zeros and ones; shape = (batch_size, 1, 28, 28)\n",
    "        \n",
    "        OUTPUT: a `torch.Distribution` instance, defined on values of shape = (batch_size, D_latent)\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # The following template might be helpful:\n",
    "#         mu = ?\n",
    "#         sigma = ?\n",
    "#         assert mu.shape == (X.shape[0], self.D_latent)\n",
    "#         assert mu.shape == sigma.shape\n",
    "#         return dist.Normal(mu, sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b69a6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a0c07280a0ec30a263f9b1fce31888f",
     "grade": false,
     "grade_id": "vae-dec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, D_latent):\n",
    "        \"\"\" \n",
    "        Initialize the Decoder `nn.Module`.\n",
    "        \n",
    "        This will operate on inputs of shape (batch_size, D_latent).\n",
    "        \n",
    "        INPUTS:\n",
    "        D_latent: size of latent space (integer)\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.D_latent = D_latent\n",
    "        \n",
    "        # Define your networks here\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, Z):\n",
    "        \"\"\" Return a distribution p(x | z) \n",
    "                \n",
    "        INPUT:\n",
    "        X    : torch.FloatTensor, real-valued, shape = (batch_size, D_latent)\n",
    "        \n",
    "        OUTPUT: a `torch.Distribution` instance, defined on values of shape = (batch_size, 1, 28, 28)\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure that the returned value has the right shape! e.g.:\n",
    "        # return dist.Bernoulli(X_hat.reshape(-1, 1, 28, 28))\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e12ea7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0ea0b1c13d9d0653ab93b6c6ec67522",
     "grade": false,
     "grade_id": "cell-2e50a8eca059aacd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# TASK 2 (5 points): Define the objective function\n",
    "\n",
    "You now need to define the ELBO, which we will maximize when estimating parameters in the encoder and decoder.\n",
    "\n",
    "## Please use a standard normal prior $N(0, I)$ for the latent space.\n",
    "\n",
    "I.e., let $$z \\sim \\mathcal{N}(0, I_D)$$ where $D$ is the dimensionality of the latent space.\n",
    "\n",
    "The `ELBO_VAE` function should return a Monte Carlo estimate of\n",
    "\n",
    "$$ELBO(X) = \\mathbb{E}_{q(Z | X)}\\left[ \\log \\left [ \\frac{p(Z)p(X | Z)}{q(Z | X)} \\right ] \\right ]$$\n",
    "\n",
    "where $q(Z | X)$ and $p(X | Z)$ are returned by the `Encoder` and `Decoder` classes you defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a50d5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52678102cd1dd0be71e247fc69166940",
     "grade": false,
     "grade_id": "vae-elbo",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ELBO_VAE(enc, dec, X):\n",
    "    \"\"\"\n",
    "    \n",
    "    INPUT:\n",
    "    enc : Instance of `Encoder` class, which returns a distribution \n",
    "          over Z when called on a batch of inputs X\n",
    "    dec : Instance of `Decoder` class, which returns a distribution \n",
    "          over X when called on a batch of inputs Z\n",
    "    X   : A batch of datapoints, torch.FloatTensor of shape = (batch_size, 1, 28, 28).\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897cbcef",
   "metadata": {},
   "source": [
    "# Train the VAE and look at the results\n",
    "\n",
    "The following code to train and run the VAE has already been written for you. \n",
    "\n",
    "You can use it to evaluate whether or not your model is working as you might expect, and for experimenting with its behavior as you change different parameters.\n",
    "\n",
    "You should't have to change the two following cells. Feel free to modify them (e.g. for adding debugging information), but be aware that I will test your implementations with substantially similar code. So, your code should be runnable by calling these cells as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e0541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try modifying this and seeing how it affects performance!\n",
    "D_latent = 2\n",
    "\n",
    "enc = Encoder(D_latent)\n",
    "dec = Decoder(D_latent)\n",
    "\n",
    "opt_vae = torch.optim.Adam(itertools.chain(enc.parameters(), dec.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epochs = 10 # Note that you may want to run more than 10 epochs!\n",
    "for epoch in range(N_epochs):\n",
    "    train_loss = 0.0\n",
    "    for (X, _) in train_loader:\n",
    "        opt_vae.zero_grad()\n",
    "        loss = -ELBO_VAE(enc, dec, X).mean()\n",
    "        loss.backward()\n",
    "        opt_vae.step()\n",
    "        train_loss += loss.item() * X.shape[0] / len(mnist_dataset)\n",
    "    print(\"Epoch %d, train loss = %0.4f\" % (epoch, train_loss));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe0e15",
   "metadata": {},
   "source": [
    "## Do reconstructions look like the input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(make_grid(X_random_batch, nrow=20).permute(1,2,0))\n",
    "plt.title(\"Input images\")\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title(\"Reconstructions\")\n",
    "with torch.no_grad():\n",
    "    plt.imshow(make_grid(dec(enc(X_random_batch).sample()).mean, nrow=20).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d907acc",
   "metadata": {},
   "source": [
    "## Do randomly generated samples look coherent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "with torch.no_grad():\n",
    "    plt.imshow(make_grid(dec(dist.Normal(0, 1).sample((140, D_latent))).mean.round(), nrow=20).permute(1,2,0), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe274ff",
   "metadata": {},
   "source": [
    "## What does the latent space look like?\n",
    "\n",
    "Note that this will only give sensible plots for very low-dimensional latent spaces. It works best if the latent space is actually 2d. Otherwise, it will plot a 2d cross-section that may or may not be informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c9e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_latent(encoder, decoder):\n",
    "    \"\"\" If this is a 2d latent space, it plots the latent space\n",
    "        If the latent space is larger than 2d, then it plots a 2d cross-section \"\"\"\n",
    "    W = torch.linalg.svd(encoder(X_random_batch).mean).Vh[:2]\n",
    "    points = dist.Normal(0, 1).icdf(torch.linspace(0.01, 0.99, 15))\n",
    "    XX, YY = torch.meshgrid(points, points)\n",
    "    XXYY = torch.stack((XX, YY)).reshape(2, -1).T\n",
    "    with torch.no_grad():\n",
    "        out = dec(XXYY @ W).mean\n",
    "    plt.imshow(make_grid(out, nrow=len(points)).permute(1,2,0))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "out = plot_2d_latent(enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc55f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to try out other things here -- for example, interpolation in the latent space between two digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32215651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932e13b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef21ee811266680dfc19586d96b4947b",
     "grade": true,
     "grade_id": "enc-test-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1a8e7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c75d8f8710e862c4c061925157135f2d",
     "grade": true,
     "grade_id": "enc-test-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8266a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ec61b41181a8689e2d7769f305245e4",
     "grade": true,
     "grade_id": "dec-test-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8188163",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "338df16d4afc0892119138f1f4d438ba",
     "grade": true,
     "grade_id": "dec-test-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e317d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "657f682300caea60a5a0c5ccca34d7e7",
     "grade": true,
     "grade_id": "ec-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5e8f3a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecd77add46c07b1b83c34deff70ed7ab",
     "grade": true,
     "grade_id": "elbo-test-1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f7da49",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "697c030af717c1aebbe60f7c56bbd838",
     "grade": true,
     "grade_id": "elbo-test-2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
